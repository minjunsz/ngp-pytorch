N_importance: 128 # number of additional fine samples per ray
N_rand: 1024 # batch size (number of random rays per gradient step)
N_samples: 64 # number of coarse samples per ray
basedir: ./logs/ # where to store ckpts and logs
chunk: 32768 # number of rays processed in parallel, decrease if running out of memory
config: None # config file path
datadir: ./data/nerf_synthetic/lego # input data directory
dataset_type: blender # options: llff / blender / deepvoxels
# expname: stability_test_float_32 # experiment name
expname: debug # experiment name
finest_res: 512 # finest resolultion for hashed embedding
half_res: True # load blender synthetic data at 400x400 instead of 800x800
i_embed: 1 # set 1 for hashed embedding, 0 for default positional encoding, 2 for spherical
i_embed_views: 2 # set 1 for hashed embedding, 0 for default positional encoding, 2 for spherical
i_img: 500 # frequency of tensorboard image logging
i_print: 100 # frequency of console printout and metric loggin
i_testset: 1000 # frequency of testset saving
i_video: 5000 # frequency of render_poses video saving
i_weights: 1000 # frequency of weight ckpt saving
log2_hashmap_size: 19 # log2 of hashmap size
lrate: 0.01 # learning rate
lrate_decay: 10 # exponential learning rate decay (in 1000 steps)
netchunk: 65536 # number of pts sent through network in parallel, decrease if running out of memory
perturb: 1.0 # set to 0. for no jitter, 1. for jitter
precrop_frac: 0.5 # fraction of img taken for central crops
precrop_iters: 0 # number of steps to train on central crops
raw_noise_std: 0.0 # std dev of noise added to regularize sigma_a output, 1e0 recommended
render_only: False # do not optimize, reload weights and render out render_poses path
sparse_loss_weight: 1e-10 # learning rate
tv_loss_weight: 1e-06 # learning rate
white_bkgd: True # set to render synthetic data on a white bkgd (always use for dvoxels)
